%!TEX root = main.tex
\section{Lattice Algorithms}
\label{se:algs}

The aim of this section is to describe a selection of lattice algorithms that are 
relevant for cryptanalysis of lattice-based cryptosystems. 

\subsection{The LLL algorithm} 

The purpose of the LLL algorithm~\cite{LeLeLo82} is to efficiently compute a basis of good quality from an arbitrary basis of an integral lattice. Formally, the quality is measured by the norm of the first  basis vector relatively to the first minimum, and efficiency refers to polynomial complexity. 

We first define the notion of LLL-reducedness and comment on its quality. We will later describe the LLL algorithm, which returns an LLL-reduced basis, and consider its run-time. 

\begin{definition}
\label{def:lll}
Let~$\delta \in (1/4,1)$. 
Let~${\bf B}$ be the matrix representation of a basis of an $n$-dimensional  lattice~$L$, and~${\bf R}$ be its R-factor. 
The basis matrix~${\bf B}$ is said $\delta$-LLL-reduced if:
\[
\forall i<j: |r_{ij}| \leq \frac{1}{2}\cdot  r_{ii}, \ \text{ and } \ \forall i <n: \delta \cdot r_{i,i}^2 \leq  r_{i+1,i+1}^2+r_{i,i+1}^2.   
\]
The first condition is called size-reduction, while the second one is attributed to Lov\'asz. 
\end{definition}

The parameter~$\delta$ is often set to~$3/4$, for the unique purpose of making quality formulas prettier. It is important to set 
it strictly less than~$1$ to allow the cost analysis to go through, but geometrically it can be thought as being equal to~$1$. In that 
case,  the Lov\'asz condition states that orthogonally to the first~$i-1$ basis vectors, the $i$-th vector is no longer than the $(i+1)$-th.

\begin{lemma}
\label{lem:lll}
Let~$\delta \in (1/4,1)$ and~$\alpha = 1/\sqrt{\delta-1/4}$. 
Let~${\bf B}$ be the matrix representation of a basis~$(\vec{b}_i)_i$ of an $n$-dimensional  lattice~$L$. If~${\bf B}$ is $\delta$-LLL-reduced, then: 
\[
\|\vec{b}_1\| \leq \alpha^{n-1} \cdot \lambda_1(L) \ \mbox{ and } \ \|\vec{b}_1\| \leq \alpha^{\frac{n-1}{2}} \cdot (\det L)^{\frac{1}{n}} .   
\]
\end{lemma}

\begin{proof}
Using the size-reduction condition for~$j=i+1$ and the Lov\'asz condition, we obtain that
\[
\forall i<n: r_{i,i} \leq \alpha r_{i+1,i+1}.
\]
As a result, the inequality~$r_{1,1} \leq \alpha^{i-1}\cdot r_{i,i}$ holds for all~$i \leq n$. Taking the product across all~$i$'s, we obtain that $r_{1,1}^n \leq \alpha^{n(n-1)/2} \cdot \prod_i r_{i,i}$. As~$\|\vec{b}_1\| = r_{1,1}$ and~$\det(L) = \prod_i r_{i,i}$ (see Lemma~\ref{le:Rdet}), we obtain the second claimed inequality. 

As~$r_{1,1} \leq \alpha^{i-1} \cdot r_{i,i}$ holds for all~$i \leq n$, we obtain that  $\min_i r_{i,i} \geq \alpha^{-n+1}\cdot  r_{1,1}$. Using Lemma~\ref{le:Rlambda} gives the first claimed inequality. 
\end{proof}

Given as input the matrix representation~${\bf B}$ of the basis~$(\vec{b}_i)_i$ of a lattice~$L$, the LLL algorithm (with parameter~$\delta \in (1/4,1)$) proceeds 
by iterating the following two steps: 
\begin{itemize}
\item[$\bullet$] \emph{Size-reduction} --- for all~$j$, for all~$i$ (in decreasing order), subtract~$\lceil r_{i,j}/r_{i,i} \rfloor \cdot \vec{b}_i$ from~$\vec{b}_j$; 
\item[$\bullet$] \emph{Swap} --- for an~$i$ such that $\delta \cdot r_{i,i}^2 >  r_{i+1,i+1}^2+r_{i,i+1}^2$, swap $\vec{b}_i$ and~$\vec{b}_{i+1}$. 
\end{itemize}
The algorithm terminates when no swap can be performed, and the current basis matrix~${\bf B}$ is returned. 
In the above, the matrix~${\bf R}$ refers to the R-factor of~${\bf B}$ and needs to be updated consistently with the updates performed on~${\bf B}$. 

\begin{lemma}
The number of loop iterations performed by the LLL algorithm is bounded from above by~$n^2 \cdot \max_i \log_{1/\delta} \|\vec{b}_i\|$,
where the~$\vec{b}_i$'s refer to the input basis. 
\end{lemma}

\begin{proof}
Let~$\Pi = \prod_i r_{i,i}^{2(n-i+1)}$. This quantity is often called the potential of the basis~$\bf{B}$. 
By rearranging terms, we observe that
\[
\Pi = \prod_{i\leq n} \prod_{j \leq i} r_{j,j}^2 =  \prod_{i\leq n} \det (\vec{B}_i^T \vec{B}_i) \ ,
\]
where~$\vec{B}_i$ is the matrix consisting of the first~$i$ columns of~$\vec{B}$. This shows that~$\Pi$ is an integer. 
Further, using the fact that the inequality~$r_{i,i} \leq \|\vec{b}_i\|$ holds for all~$i$, we obtain 
that~$\Pi \leq (\max_i \|\vec{b}_i\|)^{n^2}$. 

Now, consider a swap. Let~$\vec{R}$ and~$\vec{R}'$ respectively denote the~$R$-factor before and after the swap. 
For all~$j \notin \{i,i+1\}$, we have~$r'_{j,j} = r_{j,j}$. Also, as the determinant of the lattice spanned by the first~$i+1$ vectors is unchanged (the spanned lattice does not change), we have that~$\prod_{j \leq i+1} r'_{j,j} =  \prod_{j \leq i+1} r_{j,j}$. 
Finally, note that~$r'_{i,i} = \sqrt{r_{i,i}^2 + r_{i,i+1}^2}$. 
Overall, 
if~$\Pi$ and~$\Pi'$ respectively denote the potential before and after the swap, we obtain that:
\[ 
\frac{\Pi'}{\Pi} = \left( \frac{r'_{i,i}}{r_{i,i}}\right)^2  \leq \delta.  
\]

To sum up, at every swap, the potential decreases by a factor~$\delta$. The potential is below~$(\max_i \|\vec{b}_i\|)^{n^2}$ and always at least~1 (it is a non-zero integer). This provides the result. 
\end{proof}

The above is not sufficient to conclude that the running-time of the LLL algorithm is polynomial in the input bit-size. 
The cost is also impacted by the costs of the arithmetic opeartions. By relying on the Gram-Schmidt orthogonalization rather than the
QR-factorization, one obtains a mathematically equivalent formulation of the LLL algorithm, but all relevant quantities become rational. 
As shown in~\cite{LeLeLo82}, all the rationals involved during the computation have numerators and denominators whose bit-sizes 
are polynomial in the bit-size of the input basis. This in turn gives that this version of the LLL algorithm is polynomial-time. 
An alternative option is to rely on floating-point arithmetic, which leads to better run-times, both in theory and in practice. 
The correctness analysis of the algorithm then has to be modified to accomodate the numerical inaccuracies. We refer the interested reader to~\cite{NgSt09}. 
 
We now complete our discussion of the LLL algorithm by considering its correctness. 
All operations performed on the basis matrix are unimodular, implying that the output matrix is indeed a basis of the input lattice. 
One may check that once the size-reduction step has been performed, the first condition of Definition~\ref{def:lll} is fulfilled. 
The algorithm terminates if no swap can be performed. This is equivalent to stating that the second condition of Definition~\ref{def:lll} is satisfied. Overall, we obtain that upon termination, the LLL algorithm indeed outputs of LLL-reduced basis of the input lattice.



\subsection{Algorithms for SVP }
The main computational task associated to lattices is finding a short vector in a lattice, see Definition~\ref{def:SVP}. Recall that on input, we are given a full-rank lattice $L$ described by a matrix $B \in \RR^{n \times n}$ (with polynomially-sized entries) and some constant  $\gamma\geq 1$. The task is to output a non-zero lattice vector is a solution to the approximate shortest vector problem. Since the solution is not unique (especially for large $\gamma$), we are fine with any vector that satisfies the length condition.  

Algorithms solving SVP can be divided into two categories: provable and heuristic. The former one give guarantee that the shortest vector will be found for any input, while the latter can fail to produce the shortest vector and/or rely on assumptions that may not be satisfied by some lattices. Nevertheless, in practice we resort to heuristic algorithms. 

Another complimentary way to categorize SVP algorithms is by their design, see Table~\ref{table:SVPAlgs}. There we have four different `styles' of algorithms: Enumeration, Voronoi Cell, Gaussian Sampling, and Sieving (the latter two might be arguably merge into one category). Enumeration is the only known approach that finds the short vector with only $\poly(n)$ memory, others require at exponential in the lattice dimension space. The family of so-called sieving (or AKS) algorithms, described in the pioneering work of Ajtai, Kumar, and Sivakumar~\cite{AKS01}, offers the best known to-date heuristic algorithm for SVP. The fact that this algorithm achieves a single-exponential running time and memory complexity was already stated in the original paper~\cite{AKS01}, but a more precise analysis of the constant in the exponent has a long history. The result of Nguyen and Vidick in~\cite{NV08}, stating the running time of order $2^{5.9n+o(n)}$, was later improved by Pujol and Stehl{\'e} to $2^{2.465n+o(n)}$ running time and $2^{1.42n+o(n)}$ space~\cite{PS09}. Under an assumption on the distribution of lattice-vectors, we are able to heuristically solve appSVP in $2^{0.415n+o(n)}$ time and $2^{0.208n+o(n)}$ space using \emph{heuristic sieving}~\cite{NV08}. Finally, the currently best known running time of $2^{0.292n+o(n)}$ in~\cite{BDGL16} comes from a line of works based on the techniques from Locality Sensitive Hashing. This is to be compared with the fastest provable appSVP solver by Aggarwal et al.~\cite{ADRS15}. Based on the so-called discrete Gaussian sampling, this algorithm achieves $2^{n+o(n)}$ time and space complexity.

Both sieving and enumeration have heuristic variants  and have been implemented. Concretely,  enumeration algorithms are implemented in~\cite{fpylll}, and sieving algorithms are implemented in~\cite{g6k}. 

{
\renewcommand\arraystretch{1.4}
\begin{table}[h]
	\centering
	\begin{tabular}{| l | c | c |}
		\hline
		\textbf{Algorithm} & \textbf{Running time} & \textbf{Memory complexity} \\ \hline
		\multicolumn{3}{|c|}{\textsc{ Deterministic algorithms:} } \\ \hline
		Enumeration \cite{Kan83,HanSte07} & $n^{ (1/2e) n + \smallo(n)}$ & $\poly(n)$ \\ \hline
		Voronoi Cell \cite{MicVou10} & $2^{2n + \smallo(n)}$ & $2^{n+\smallo(n)}$ \\ \hline
		\multicolumn{3}{|c|}{\textsc{ Probabilistic algorithms:} } \\ \hline
		Gaussian Sampling \cite{ADRS15} & $2^{n+ \smallo(n)}$ & $2^{n+ \smallo(n)}$ \\ \hline
		Sieving \cite{AKS01} & & \\ [-1ex]
		\hspace{5pt} -- Provable \cite{PS09} & $2^{2.465n + \smallo(n)}$& $2^{1.325n + \smallo(n)}$ \\ [-1ex]
		\hspace{5pt} -- Heuristic \cite{BDGL16}: & $2^{0.292n + \smallo(n)}$ & $2^{0.208n + \smallo(n)}$  \\  \hline
	\end{tabular}
	\caption[Algorithms for SVP]{Algorithms for SVP on an $n$-dimensional lattice} 
	
	\label{table:SVPAlgs}
\end{table} 

}
\subsubsection{Enumeration}

The first known algorithms for SVP belong to what is now known as \emph{enumeration} algorithms. The name describes the main idea: it enumerates lattice points within a small convex body. The idea belongs to Fincke-Pohst~\cite{FP83} and in cryptography it is more often referred to as Kannan's enumeration~\cite{Kan83}. 

The algorithm finds the shortest vector in the lattice given by a basis $B$ and we assume that we know $\lambda_1(L(B))$. It uses the $R$-factor of $B$ which can be obtained in $\poly(n)$ time. Let us fix the value $k \in \RR$, for example, $k = \| \vec b_1 \|$ -- the norm of the first basis vector. Enumeration will find all integral vectors $\vec x \in \ZZ^n$ such that $\|B\vec x \| < k$. Let us see what this condition means for the $R$-factor of $B$.

\begin{align}\label{eq:enum_norm}
	\| B\vec x\|^2  = \| R\vec x\|^2 = \| \left( \sum_{i=1}^n r_{1,i}x_i, \sum_{i=2}^n r_{2,i}x_i, \ldots, r_{n,n} x_n \right) \|^2 = \sum_{j=1}^n \left( \sum_{i \geq j} r_{j,i} x_i \right)^2.
\end{align}

Now if we aim for $\vec x \in \ZZ^n$ such that $\| B \vec x\|^2 < k^2$, then it should hold that $(r_{n,n} x_n)^2 < k^2$. Since $x_n$ should be integral, the only options we have are $|x_n| < \frac{k} {r_{n,n}}$. That is, there are $2 \frac{k}{r_{n,n}}+1$ possible values for $x_n$ in total.

On the second step, we fix any legitimate value for $x_n$ and consider the last two addends in the sum from~(\ref{eq:enum_norm}):
\[
(r_{n-1, n-1}x_{n-1} + r_{n-1, n}x_n)^2 + (r_{n,n}x_n)^2 < k^2 \iff \left|{x_{n-1} + \frac{r_{n-1,n}}{r_{n-1,n-1}}x_n }\right| < \left( \frac{k^2-(r_{n,n}x_n )^2}{r_{n-1, n-1}}\right)^{1/2}.
\]

For fixed $x_{n}$, we have that possible values for $x_{n-1}$ that satisfy the above belong to an interval of length $< \frac{2k}{r_{n-1, n-1}}+1$.

Continuing this way, we receive a tree-like algorithm, where the nodes of the tree are the possible coefficients $x_{i}$ whose parent is a fixed coefficient $x_{i+1}$ (the tree is of depth $n$ having values for $x_n$ on the top). Traversing such a tree can be done in the depth-first manner: from $x_n$ down to $x_1$ -- a path that gives a candidate for the shortest vector $B\vec x$. We do not have to store all the tree, but only keep the path that leads to the currently shortest vector.

\begin{lemma} The size of the enumeration tree of the above algorithm that receives on input an LLL-reduced basis $B$	is $2^{\O(n^2)}$. 
\end{lemma}

\begin{proof}
	Since the input basis in LLL-reduced, it holds that $r_{1,1}/r_{i,i} \leq \alpha^{i-1}$, where $\alpha$ is a parameter of LLL (see~Lemma~\ref{lem:lll}). For $k = r_{1,1} = \| \vec b_1 \| < \alpha^{\frac{n-1}{2}} (\det L)^{1/n}$, we have
	
	\begin{align*}
		|\texttt{Enum.tree}| = \sum_{j=1}^n \prod_{i \leq j} \left( \frac{2k}{r_{j,j}} +1 \right) \stackrel{\alpha=2}{=} \sum_{j=1}^n \prod_{i \leq j} (2^i+1)\leq \sum_{j=1}^n \prod_{i \leq j} 3^i \leq n \cdot  \prod_{i \leq j} 3^i = n \cdot 3^{n^2}.
	\end{align*}

On each node the algorithm spends at most $\poly(n)$ time, hence the overall complexity (dominated by the size of the tree) is as in the statement.
\end{proof}

The smaller the $r_{i,i}$'s are, the `wider' the tree is and the slower the algorithm is. It turns out that there exists a preprocessing of the input basis (much stronger than LLL) that makes the last (the smallest) $r_{i,i}$'s large~\cite{Kan83,HanSte07}. Such preprocessing allows to shrink the enumeration tree to the size $n^{\frac{n}{2e}+o(n)}$, while the memory is kept polynomial in $n$.

\subsubsection{Heuristic Sieving}

Let us define some geometric objects that will be useful for the analysis of sieving algorithms.

Denote by $S^{n-1}:=\{ \vec x \in \RR^n \; : \; \|\vec x \| = 1 \}$ the unit sphere and by $\mathcal{H}_{\vec v, \alpha}:=\{\vec x \in \RR^n \; : \; \langle\vec v, \vec x \rangle \geq \alpha\}$ a half-space defined for any $\vec v \in \RR^n$ and $0 \leq \alpha \leq 1$.

\begin{definition}[Spherical cap]
	For a $\vec v \in \RR^n$ and $0 \leq \alpha \leq 1$, the spherical cap $\mathcal{C}_{\vec v, \alpha}$ is
	\[
		\mathcal{C}_{\vec v, \alpha} := S^{n-1} \cap \mathcal{H}_{\vec v, \alpha}.
	\]
	
\end{definition}

A proof of the following lemma is elementary and can be found in~\cite[Lemma~A.3]{MV10}. 
\begin{lemma}{[Volume of $\mathcal{C}_{\vec v, \alpha}$, \cite[Lemma~A.3]{MV10}]} \label{lem:cap_vol}
	Let $\mathcal{V}()$ be the volume operator. Then for any $\vec v \in \RR^n$,
	\[
		\frac{\mathcal{V} (	\mathcal{C}_{\vec v, \alpha} ) }{\mathcal{V}(S^{n-1})} = \poly(n) \left( \sqrt{1-\alpha^2} \right)^n.
	\]

\end{lemma}

\paragraph{The Nguyen-Vidick sieve.} Sieving algorithms have two flavors: the Nguyen-Vidick sieve \cite{NV08} and the Gauss sieve \cite{MicVou10}. Both make $\poly(n)$ number of calls to an oracle that given a list $\mathcal{L}$ of lattice vectors finds all pairs $\vec{x}_1, \vec{x}_2$ from  this list such that $\| \vec{x}_1 \pm \vec{x}_2 \| < \min{ \| \vec{x}_1, \vec{x}_2\| }$. 

The Nguyen-Vidick sieve starts by sampling lattice-vectors $\vec x \in L(\mathbf{B}) \cap \mathcal{B}(\mathbf{0}, 2^{\mathcal{O}(n)} \cdot \lambda_1(B))$. This can be done using, for example, Klein's sampling procedure~\cite{Klein00} that outputs a lattice-vector of length not greater than $2^{\mathcal{O}(n)} \cdot \lambda_1(B)$.  In the $2$-List Nguyen-Vidick sieve, we sample many such lattice-vectors, put them in a list $L$, and search for \emph{pairs} $\vec x_1 \times \vec x_2 \in \mathcal{L} \times \mathcal{L}$ s.t. $\| \vec x_1 \pm \vec x_2 \| \leq (1-\varepsilon) \max\{\| \vec x_1 \|, \| \vec x_2 \|\}$ for some small $\varepsilon>0$. The sum is put into the output list $\mathcal{L}_{\mathtt{out}}$. The size of $L$ is chosen in a way to guarantee $|{\mathcal{L}}| \approx |\mathcal{L}_{\mathtt{out}}|$. The search for pairs is repeated over the list $\mathcal{L}_{\mathtt{out}}$ once it is large enough.  The process is formalized in Algorithm~\ref{algo:NVsieve}.


\begin{algorithm}
	\caption{Nguyen-Vidick sieve~\cite{NV08}}
	\label{algo:NVsieve}
	\begin{algorithmic}[1]
		\REQUIRE A lattice $L$ given be an LLL-reduced basis $B$, $0 < \varepsilon < 1$;
		\ENSURE $\vec v \in L$ such that $\|\vec v\| \approx  \lambda_1(L)$  \vspace{1mm}
		\STATE $\mathcal{L} \gets$ \texttt{Sample}$(L(B)$)  \COMMENT{Use sampler from~\cite{Klein00} }
		\WHILE {$ |\mathcal{L}[0]| < \lambda_1(L)$}
		\STATE $\mathcal{L} \gets $ 	\texttt{$2$-Sieve} on $\mathcal{L}, \varepsilon$
		\STATE Sort $\mathcal{L}$ in increasing length
		\ENDWHILE
	\end{algorithmic}

	\vspace{5mm}

	\texttt{$2$-Sieve}
	\begin{algorithmic}[1]
		\REQUIRE  a list of lattice vectors $\mathcal{L}$ , $0 < \varepsilon < 1$;
		\ENSURE  a list of shorter lattice vectors $\mathcal{L}_{\mathtt{out}} $ \vspace{1mm}
		\STATE $\mathcal{L}_{\mathtt{out}} \gets \emptyset$
		\FOR{$\vec x_1, \vec x_2 \in L$}
		\IF{$\| \vec x_1 \pm \vec x_2 \| \leq (1-\varepsilon) \max\{\| \vec x_1 \|, \| \vec x_2 \|\}$}
		\STATE $\mathcal{L}_{\mathtt{out}} \gets \mathcal{L}_{\mathtt{out}}  \cup \{ \vec x_1 \pm \vec x_2  \}$ \COMMENT{choose the sign that gives the shorter vector}		
		\ENDIF
		\ENDFOR
		\RETURN $\mathcal{L}_{\mathtt{out}}$
	\end{algorithmic}

\end{algorithm} 

The size of $\mathcal{L}$ determines the space complexity of the algorithm. To obtain an estimate of the required $|\mathcal{L}|$,~\cite{NV08} propose the following heuristic.

\begin{heuristic} \label{heur:sieve_heuristic}
	At any stage of~Algorithm~\ref{algo:NVsieve}, the vectors in the list $\mathcal{L}$
	are uniformly distributed on the spherical  $S^{n-1}$.
\end{heuristic}

The assumption is that the sieving algorithm operates on list of vectors that  lie uniformly on the unit sphere. It may sound contradictory at first sight, but in high dimensions the length of vectors sampled under the Gaussian distribution are concentrated around the expectation and the heuristic makes this concentration statement stronger. The heuristic here is that it does not affect the behavior of the algorithm. Intuitively, the discreteness of a lattice should not be `visible’ to the algorithm (at least not in the search for the approximate shortest vector; as soon as we see the discreteness, the vectors are already short enough).

\begin{theorem} \label{thm:NVsieve}
	Under Heuristic~\ref{heur:sieve_heuristic}, Algorithm~\ref{algo:NVsieve} outputs in time $2^{0.415n + o(n)}$ a vector $\vec v$ of norm $\| \vec v \| \approx \lambda_1(L)$ using $2^{0.2075n + o(n)}$ memory.
\end{theorem}

\begin{proof}
	Under Heuristic~\ref{heur:sieve_heuristic} the list $\mathcal{L}$ consists of uniform random elements from the sphere $S^{n-1}$. In the process of sampling uniform random vectors $\vec x_i$ on $S^{n-1}$, observe that any pair $\vec x_1, \vec x_2 \in S^{n-1}$ that satisfies $\langle \vec x_1, \vec x_2 \rangle <1/2$ gives raise to a shorter vector. Concretely, $\| \vec x_1 - \vec x_2 \|^2 = \| \vec x_1 \|^2 -2\langle \vec x_1, \vec x_2 \rangle +\| \vec x_1 \|^2 < 1 - 1 + 1 < 1$. Hence, each sampled vector $\vec x_1$ will give raise to a shorter vector whenever there is another vector landing in $\mathcal{C}_{\vec x_1, 1/2}$. Therefore, after sampling $\poly(n) \frac {\mathcal{V}(S^{n-1})} {\mathcal{V} (	\mathcal{C}_{\vec v, \alpha} ) }$ random vectors from $S^{n-1}$ we expect to cover all the sphere with the caps $\mathcal{C}_{\vec x_1, 1/2}$ and each next sample will result in a shorter vectors. Therefore, we need to start $| \mathcal{L}| = \poly(n) \cdot(\sqrt{1-1/4})^{-n} = \poly(n) \left(\sqrt{\frac{4}{3}}\right)^{n}\approx 2^{0.2075n + o(n)}$ vectors in order to expect the same number of shorter vectors to be constructed.
	
	As we start with $2^{\O(n)}$ approximation to the shortest vector (guaranteed by the Klein's sampler) and at each step we decrease the norm by a small constant $\varepsilon$, we expect after $\poly(n)$ iterations of the While-loop on line~2 of Algorithm~\ref{algo:NVsieve} to have a list of vectors that contains the shortest vector.
	
	The lists sizes determine the memory of the algorithm. The runtime is dominated by the For-loop in 	\texttt{$2$-Sieve} (line~2) that search for pairs from the list of size $2^{0.2075n + o(n)}$, which can be done in time the list size squared.
	
\end{proof}

The search for pairs with certain angular distance (i.e., the inner product) is done naively in the Nguyen-Vidick sieve. The so-called near-neighbor techniques can significantly speed up this process reaching the currently best sieving algorithm~\cite{BDGL16}

\paragraph{The Gauss sieve}
 
In the Gauss Sieve~\cite{MV10}, we keep a set $\mathcal{S}$ of vectors and try to perform as many pair-reductions on $\mathcal{S}$ as possible. Again, we look for pairs $(\vec x_1, \vec x_2)\in \mathcal{S}$ that produce shorter sum.

To avoid checking the same paimultiple times, we separate $\mathcal{S}=\mathcal{L} \cap \mathcal{Q}$ into a list $=\mathcal{L} $ and a queue $\mathcal{Q}$. The list $\mathcal{L}$ contains the lattice vectors that we already checked: we maintain that no pair from $\mathcal{Q}$ can produce a shorter sum. The queue $\mathcal{Q}$ contains vectors that might still produce a shorter sum.
 
Due to our splitting of $\mathcal{S}$, we may assume that one of the vectors in the `good' pair is from $\mathcal{Q}$.Whenever we sample or modify a vector, we have to move it into $\mathcal{Q}$; if no more reduction is possible with $\vec x_1 \in \mathcal{S}$, we move $\vec x_1$ from $\mathcal{Q}$ into $\mathcal{L}$.  If $\mathcal{Q}$ is empty, this signals that we have to sample a new vector.
Since the lengths of the vectors in $\mathcal{S}$ keeps decreasing, we hope to eventually find the shortest vector.We stop the search when the length of the shortest element of the list (i.e., the shortest lattice vector found) is equal to the first successive minimum, $\lambda_1(L)$.
Since we usually do not know the value of $\lambda_1(L)$ exactly, we use some heuristics:  in practice, we stop once we either found a lot of pairs that would give a zero vector or we are sufficiently close to the Minkowski bound.

The analysis of the Nguyen-Vidick sieve can be adapted to the Gauss sieve so that the statement of Theorem~\ref{thm:NVsieve} applied to the latter as well.
 
 

\subsection{The BKZ algorithm} 

The acronym BKZ stands for `Blockwise-Korkine-Zolotarev'. It is an algorithm proposed by  Schnorr and Euchner in~\cite{SchE94}, but prior to that Schnorr~\cite{S87} introduced the concept of BKZ reduction as a generalization of LLL.

Block reduction allows you to find somewhat short vectors in a lattice. We just saw that finding the shortest vector in a lattice (i.e. solving SVP) is  hard (known algorithms take at least $2^{\Omega(n)}$ time). On the other hand, finding somewhat short vectors that are longer than the shortest vector by an exponential factor takes only $\poly(n)$ time (LLL). In cryptanalysis, we are actually interested in the problem `in-between' as the security of all known lattice-based primitives relies on the hardness of approximate SVP with $\poly(n)$ approximation factor. This is what BKZ algorithm is for: parameterized by a block-size $2 \leq \beta \leq n$ it solves several instances of SVP on projected sublattices of the input lattice $L$.
The main intuition behind BKZ is that the larger the block-size $\beta$ is, the more expensive the algorithm is in terms of running-time, but the `better' the quality of the output basis.


Block reduction gives us more than just short vectors. It gives us guarantees on the “quality” of the basis, namely the size of $r_{i,i}$'s -- the main diagonal of the $R$-factor. Recall from the analysis of the enumeration algorithm for SVP that we need these $r_{i,i}$'s to not  not drop off too quickly. Since the product of all $r_{i,i}$'s is fixed  (and is equal to the determinant of the lattice), it is easy to see that if the $r_{i,i}$'s do not drop off too quickly, the first vector can be shown to be relatively short.

\paragraph{How BKZ works.} In order to ensure that the $r_{i,i}$'s do not drop off to quickly, we are going to reduce them locally. For that, we will work on projected lattice blocks. Concretely, for given a basis $B$, denote by $B_{[i,j]}$ for $i<j$ the basis formed by the basis vectors $\vec b_i, \ldots \vec b_j$ projected orthogonally to the first $i-1$ basis vectors. That is $B_{[i,j]}$  is a basis for the lattice given by the sublattice formed by $\vec b_1, \vec b_2, \ldots ,\vec b_j$ projected onto the orthogonal subspace of the vectors $\vec b_1,\vec b_2, \ldots ,\vec{b}_{i-1}$. Notice that $\tilde{\vec{b}}_i$ the first vector of $B_{[i,j]}$. 

Alternatively, we can look at projective sublattice using the $R$-factor, which is an upper triangular matrix.  The projection of a basis vector orthogonal to the previous basis vectors  corresponds to  dropping the first entries from the vector. So considering a projected block $R_{[i,j]}$ is the same as to consider the square submatrix of $R$ consisting of the rows and columns with indices between $i$ and $j$ (inclusive).

To control the sizes of $r_{i,i}$'s,  we call an SVP solver (either enumeration or sieving) on the block $B_{[i,j]}$ of dimension $\beta = j-i+1$. The idea is that once this oracle finds the shortest vector in $B_{[i,j]}$ it inserts in into the basis $B$ on the $i$-th position and removed potential linear dependencies using LLL. Algorithm~\ref{algo:BKZ} formulates this idea.

\begin{algorithm}
	\caption{BKZ}
	\label{algo:BKZ}
	\begin{algorithmic}[1]
		\REQUIRE An LLL-reduced lattice basis $B$, block parameter $\beta$, and an access to SVP oracle
		\REPEAT
		\FOR{$i\gets 1$ to $n$}
 		\STATE Run LLL on $B_{[i,\max\{i+\beta-1, n\}]}$
 		\STATE $\vec v \gets$ the shortest vector in $B_{[i,\max\{i+\beta-1, n\}]}$
 		\STATE insert $\vec v$ into $B$
 		\ENDFOR
		\STATE \UNTIL{no change occurs}
	\end{algorithmic}
\end{algorithm} 

Algorithm~\ref{algo:BKZ} starts by calling the SVP oracle on the first block $B_{[1,\beta]}$. Once this block is SVP reduced, we move to the next block $B_{[2,\beta+1]}$ and call the oracle on that. Notice that SVP reduction of $B_{[2,\beta+1]}$ together with insertion of the new vector and LLL may change the projected lattice $B_{[1,\beta]}$ and $\vec b_1$ may not be the shortest vector in the first block anymore, that is it can potentially be reduced even further. We deal with it later and continue in this fashion until we reach the end of the basis, i.e. until we called the oracle on $B_{[1,\beta]}$.  Once we have reached the end of the basis, i.e., we are finished with $B_{[n-\beta+1,n]}$, we start reducing the window size, i.e. we call the oracle on $B_{[{n-\beta},n]}$, $B_{[n-\beta+1,n]}$, etc., and we stop after we are finished with $B_{[n-1,n]}$. This whole process is called a BKZ tour.

Once we have finished a tour, we go back and fix the blocks that are not SVP reduced anymore (the Repeat-Until in Algorithm~\ref{algo:BKZ}).  Again, if the second tour modified the basis, there is no guarantee that all the blocks are SVP reduced and we go for the third tour. In principal, this process can be unbounded, but~\cite{HPS11} show that terminating BKZ after $\poly(n)$-many tours will not  impact the quality of the output basis.

\paragraph{Quality of BKZ-reduced basis.} Let us recall statement of Minkowski's theorem (Theorem~\ref{th:Minko}): for any $n$-dimensional lattice $L$, we have $\lambda_1(L) \leq \sqrt{n}\det(L)^{1/n}$. Applying this theorem to the SVP reduced projected block $B_{[i, i+\beta-1]}$, obtain for all $i$
	\[
		\| \tilde{\vec{b}}_i \| \leq \sqrt{\beta} \left( \prod_{j=1}^{i+\beta-1} \| \tilde{\vec{b}}_j \|  \right)^{1/\beta}.
	\]
Alternatively, write $\| \tilde{\vec{b}}_i \|^\beta \leq \beta^{\beta/2}  \prod_{j=1}^{i+\beta-1} \| \tilde{\vec{b}}_j \|  $. If we multiply all these inequalities together, we obtain 
\[
	\| \tilde{\vec{b}}_1 \|^{\beta-1} \cdot \| \tilde{\vec{b}}_2 \|^{\beta-2} \cdot \ldots \cdot \| \tilde{\vec{b}}_{\beta-1} \| \leq 
	\beta^{\frac{\beta(n-\beta+1)}{2}} \| \tilde{\vec{b}}_{n-\beta+2} \|^{\beta-1} \| \tilde{\vec{b}}_{n-\beta+3} \|^{\beta-2} \cdot \ldots \cdot  \|  \tilde{\vec{b}}_n\|.
\]

Using the fact that not only $B_{[i, i+\beta-1]}$ are SVP-reduced, but also its sublattices $B_{[i, j]}$  for $j<i+\beta-1$, it holds that
\[
	\|  \tilde{\vec{b}}_1   \|^j \leq j^{j/2} \prod_{\ell = 1}^{j} \| \tilde{\vec{b}}_j  \|.
\]

Combining the above with $\|  \tilde{\vec{b}}_1  \| \leq \|  \tilde{\vec{b}}_1  \| $, obtain
\[
	\|  \tilde{\vec{b}}_1   \|^{\beta(\beta-1)/2} \leq \left( \prod_{i=2}^{\beta-1} i^{i/2} \right) \prod_{i=1}^{\beta-1} \|  \tilde{\vec{b}}_i  \|^{\beta-1}.
\]

Since $\beta^\beta \geq i^i$ for $i \leq \beta$, we finally have
\[
	\|  \tilde{\vec{b}}_1   \|^{\beta(\beta-1)/2} \leq \beta^{\beta(n-1)/2}  	\|  \tilde{\vec{b}}_{n-\beta+2}   \|^{\beta-1} \|  \tilde{\vec{b}}_{n-\beta+2}   \|^{\beta-2} \cdot \ldots \cdot  \|  \tilde{\vec{b}}_n\|.
\]

Assuming that for the shortest vector $\vec v$ its projection orthogonal to the first $n-1$ basis vectors is non-zero, we have that $\lambda_1(L) = \| \vec v \| \geq \| \tilde{\vec{b}}_i\|$ for $n-\beta+2\leq i\leq n$. From here is follows that $\lambda_1(L) \geq \max_{n-\beta+2\leq i\leq n}\{ \| \tilde{\vec{b}}_i\|\}$. The approximation factor achieved by BKZ now follows:
\[
	\|  \tilde{\vec{b}}_1   \| \leq \beta^{\frac{n-1}{\beta-1}}\lambda_1(L).
\]

\paragraph{Runtime} The running time of the algorithm is dominated by the SVP calls (if we bound the number of tours by $\poly(n)$). That is, the complexity of BKZ is driven my the complexity of SVP on lattices of dimension $\beta$, which is at least $2^{\O(\beta)}$.